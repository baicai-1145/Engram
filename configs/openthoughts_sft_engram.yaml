# OpenThoughts3-1.2M (local parquet) + Engram (single GPU) starter config.

seed: 0
trust_remote_code: true
model_name_or_path: ./Qwen3-0.6B-Base

data:
  format: parquet
  mode: sft
  seq_len: 2048
  train_files:
    - OpenThoughts3-1.2M/data/train-*.parquet
  eval_files: []

  # OpenThoughts schema:
  conversation_field: conversations
  conversation_from_field: from
  conversation_value_field: value
  # If you have enough RAM, set true to reduce random IO.
  keep_in_memory: false
  # Optional: set to a fast local SSD path to speed up datasets cache.
  hf_cache_dir: ""
  # Bounded-RAM mode: stream parquet, keep a shuffle buffer in RAM.
  # This approximates "load a chunk into memory, train, then slide forward".
  streaming: true
  # Tune this to your RAM. Rough guide:
  # - start with 5_000~50_000 and scale up while monitoring RSS.
  shuffle_buffer_size: 20000
  streaming_seed: 0

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.0
  warmup_steps: 50
  max_steps: 1000000
  lr_scheduler_type: cosine
  logging_steps: 10
  # TensorBoard (Requires: pip install tensorboard)
  report_to: ["tensorboard"]
  save_steps: 100
  eval_steps: 0
  save_total_limit: 10
  bf16: true
  fp16: false
  gradient_checkpointing: true
  # Ablation / speedup: train only `.engram.*` params (freeze Qwen weights).
  # Note: still backprops through the frozen backbone to update Engram.
  train_only_engram: true
  # Detailed Engram stats (per layer): gate distribution + output RMS ratio.
  log_engram_stats: true
  # Quantiles are computed on a prefix sample of gate values to keep overhead bounded.
  engram_stats_sample_tokens: 4096
  # Expensive weight norms (including huge embedding weights). Use a low frequency.
  weight_stats_every: 100
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  dataloader_prefetch_factor: 4
  # Reduce checkpoint IO stalls.
  save_only_model: true

engram:
  enabled: true
  layer_index_base: 0
  layers: [2, 15]
  max_ngram: 3
  num_heads: 8
  table_size_per_ngram: [100000, 100000]
  tokenizer_compression_mode: demo
  gating_mode: paper
  init_equivalence: zero_output
  lr_scale: 5.0
  cache_dir: .cache/engram
